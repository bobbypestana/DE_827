{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35145e3d",
   "metadata": {},
   "source": [
    "# Apache Hive\n",
    "É um sistema de data warehouse de código aberto, usado exclusivamente para consultar e analisar grandes conjuntos de dados armazenados no Hadoop.\n",
    "O Hive utiliza uma linguagem chamada HiveQL (Hive Query Language) , que transforma as sentenças SQL em Jobs MapReduce executados no cluster Hadoop, para assim aproveitar as \"skills\" em SQL dos analistas e desenvolvedores do Facebook, que não eram na época tão proficientes em Java para usar o MapReduce.\n",
    "\n",
    "## O que é?\n",
    "- Interpreta instruções SQL para Job MapReducer\n",
    "- Lê dados de arquivos estruturados e semi-estruturados no HDFS, e se baseia em metadados para simular uma tabela de um banco de dados relacional \n",
    "- Não possui um SGBD\n",
    "- Foi desenhado para melhor performance analisando grandes quantidades de dados que se encontram em clusters.\n",
    "- Data Warehouse do Hadoop\n",
    "\n",
    "## Hive x RDBMS\n",
    "|                              |     HIVE                    |     RDBMS                               |\n",
    "|------------------------------|-----------------------------|-----------------------------------------|\n",
    "|     Uso                      |     Foco   em analytics     |     Foco   em on line ou   analytics    |\n",
    "|     Acesso                   |     Batch                   |     Batch   e Interativo                |\n",
    "|     Integridade              |     Baixa                   |     Alta                                |\n",
    "|     Escalabilidade           |     Horizontal              |     Vertical                            |\n",
    "|     Armazenamento            |     Baixo   custo por PB    |     PB?                                 |\n",
    "|     Interface                |     HiveQL                  |     SQL                                 |\n",
    "|     Latência                 |     Minutos   ou mais       |     ms,   ml ou segundos                |\n",
    "|     Estrutura   de dados     |     Não   Estruturado       |     Estruturado                         |\n",
    "\n",
    "## Arquitetura\n",
    "![Arquitetura](https://s3-sa-east-1.amazonaws.com/lcpi/c9506eb7-cbf9-4b2b-848d-284e36e363bc.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aaa044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5260f32d",
   "metadata": {},
   "source": [
    "# Componentes do Hive\n",
    "## Driver\n",
    "Compila, otimiza, executa o HiveQL, e decide se executa a query local ou submeter em um job MapReduce.\n",
    "\n",
    "## Metastore\n",
    "Armazena os metadados, interpretando dos arquivos no HDFS como conteúdos de uma tabela. Armazena as informações de como as linhas e colunas são delimitadas dentro do arquivo (Hive Schemas). Pode ser armazenado no MySQL, Oracle, Derby ou Postgresql.\n",
    "\n",
    "## CLI\n",
    "Command Line Interface – para acessar o shell do hive. É o serviço padrão. Acessado pelo terminal no SO.\n",
    "\n",
    "## Hiveserver\n",
    "Permite que conexões (Thrift, ODBC e JDBC) de outros componentes tenham comunicação com o hive.\n",
    "\n",
    "## Hiveserver2\n",
    "Evolução do hiveserver, suportando autenticação e múltiplos usuários concorrentes.\n",
    "\n",
    "## HiveQL\n",
    "Utiliza Data Definition Language (DDL) para criar os bancos de dados e tabelas.\n",
    "\n",
    "## Beeline\n",
    "É a Command Line Interface para acessar o hiveserver2, utilizando conexão JDBC.\n",
    "\n",
    "Execute comando beeline para iniciar a interface de consulta:\n",
    "    \n",
    "    beeline -u \"jdbc:hive2://quickstart.cloudera:10000/default\"\n",
    "    \n",
    "Após iniciar beeline, execute consulta no hive:\n",
    "\n",
    "    show tables;\n",
    "\n",
    "Para poder ver os comandos do beeline:\n",
    "\n",
    "    beeline -help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f9aadf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "609784a7",
   "metadata": {},
   "source": [
    "# Tabelas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4cc588",
   "metadata": {},
   "source": [
    "## Tabela Gerenciada (Manage Table)\n",
    "Também conhecidas como **tabelas internas**, tem o ciclo de vida de seus dados controlados pelo Hive.\n",
    "\n",
    "Armazena os dados dessas tabelas em um subdiretório sob o diretório definido por hive.metastore.warehouse.dir (por exemplo, / user / hive / warehouse) \n",
    "\n",
    "Lembre-se: quando excluímos a tabela os dados também são excluídos!\n",
    "\n",
    "### Prática!\n",
    "1. Criar diretório `/FileStore/tables/hive_aula` no hdfs.\n",
    "\n",
    "    \n",
    "    hdfs dfs -mkdir /FileStore\n",
    "    hdfs dfs -mkdir /FileStore/tables\n",
    "    hdfs dfs -mkdir /FileStore/tables/hive_aula/\n",
    "\n",
    "\n",
    "\n",
    "2. Crie um arquivos chamado `tabela_interna.txt`, escreva algum conteúdo .\n",
    "\n",
    "    1. vim tabela_interna.txt  \n",
    "    2. insira algum texto interno  \n",
    "    3. Esc -> :wq\n",
    "    \n",
    "ou use a `interface gráfica`.\n",
    "\n",
    "3. Insira o arquivo no diretório do hive.\n",
    "\n",
    "        hdfs dfs -put tabela_interna.txt /FileStore/tables/hive_aula/\n",
    "\n",
    "ou use a `interface gráfica`.\n",
    "\n",
    "\n",
    "4. Crie a tabela e faça o upload dos dados.\n",
    "\n",
    "        CREATE TABLE ex_interna(texto_livre STRING);\n",
    "\n",
    "        LOAD DATA INPATH '/FileStore/tables/hive_aula/tabela_interna.txt' INTO TABLE ex_interna\n",
    "\n",
    "5. Consulte a tabela.\n",
    "\n",
    "        select * from ex_interna;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13390a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e815c304",
   "metadata": {},
   "source": [
    "## Tabela Externa(External Table)\n",
    "\n",
    "External Table, é apenas uma referência a um arquivo, ou seja, o Hive não é dono dos dados. É possível, inclusive, que uma ou mais external tables apontem para o mesmo arquivo no HDFS.\n",
    "\n",
    "Neste caso, quando excluímos a tabela os **dados não são excluídos**, apenas seus metadados! \n",
    "\n",
    "Por fim, é importante se atentar que é necessário informar a localização dos dados. \n",
    "\n",
    "A palavra-chave EXTERNAL diz ao Hive que esta tabela é externa  \n",
    "Cláusula LOCATION é necessária para informar ao Hive onde os dados estão localizados\n",
    "\n",
    "### Prática!\n",
    "1. Criar diretorio no hdfs `/FileStore/tables/hive_aula_ext`.\n",
    "  \n",
    "        hdfs dfs -mkdir /FileStore/tables/hive_aula_externa\n",
    "  \n",
    "2. Crie o arquivo `tabela_externa.txt` no seu computador\n",
    "\n",
    "        1. vim tabela_externa.txt  \n",
    "        2. insira algum texto interno  \n",
    "        3. Esc -> :wq\n",
    "\n",
    "ou use a `interface gráfica`.\n",
    "    \n",
    "    \n",
    "3. Crie a tabela com o comando do hiveQL.\n",
    "\n",
    "       CREATE EXTERNAL TABLE ex_externa(texto_livre string) \n",
    "       LOCATION '/FileStore/tables/hive_aula_externa';  \n",
    "\n",
    "\n",
    "4. Consulte a tabela\n",
    "\n",
    "        select * from ex_externa\n",
    "\n",
    "5. Copie o arquivo aula para novo diretório \n",
    "\n",
    "        hdfs dfs -rm /FileStore/tables/hive_aula_externa/tabela_externa.txt\n",
    "\n",
    "6. Execute novamente a query a visualizar\n",
    "\n",
    "        select * from ex_externa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2203553b",
   "metadata": {},
   "source": [
    "## Location\n",
    "\n",
    "Tabelas criadas sem LOCATION, os dados são armazenados no diretório default do HDFS hive que é: \n",
    "~~~\n",
    "/user/hive/warehouse/NOME_DATABASE.db/NOME_TABELA \n",
    "~~~\n",
    "\n",
    "Quando é criado no database default a estrutura é: \n",
    "~~~\n",
    "/user/hive/warehouse/NOME_TABELA \n",
    "~~~\n",
    "\n",
    "Tabelas criadas com LOCATION, o arquivo é armazenado neste local definido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0cefbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "296fac9e",
   "metadata": {},
   "source": [
    "# Database\n",
    "- Banco de dados no Hive é essencialmente apenas um catálogo ou espaço para tabelas .\n",
    "- Se não especificar o banco de dados, o banco de dados `default` é usado. \n",
    "- Podemos criar vários bancos de dados .\n",
    "- Podemos alterar o diretório na hora da criação.\n",
    "\n",
    "## Comandos\n",
    "Criar banco de dados\n",
    "~~~ \n",
    "create database if not exists [nome] location [path no hdfs] ;\n",
    "~~~\n",
    "\n",
    "Visualizar banco de dados\n",
    "~~~\n",
    "show databases;\n",
    "~~~\n",
    "\n",
    "Definição do banco de dados \n",
    "~~~\n",
    "describe database [nome];\n",
    "~~~\n",
    "Excluir banco de dados \n",
    "~~~\n",
    "drop database [nome];\n",
    "~~~\n",
    "\n",
    "Alterar banco de dados\n",
    "~~~\n",
    "alter database [nome] [parâmetros];\n",
    "~~~\n",
    "\n",
    "### Prática!\n",
    "1. Crie um banco de dados `aula` na pasta `/user/hive/db_aula` no Hive \n",
    "\n",
    "        create database aula location '/user/hive/db_aula'\n",
    "    \n",
    "2. Verifique o database\n",
    "\n",
    "        show databases\n",
    "\n",
    "3. Definição do banco de dados\n",
    "\n",
    "\n",
    "        DESCRIBE DATABASE aula\n",
    "\n",
    "\n",
    "## View\n",
    "\n",
    "É uma referência lógica de uma consulta, é como salvar uma query em uma tabela lógica, para ser acessada de forma transparente como se fosse uma física. Muito utilizada para consultas preestabelecidas por usuários e controle de acesso de campos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae541c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7b81503",
   "metadata": {},
   "source": [
    "\n",
    "# Funções\n",
    "O Hive disponibiliza funções nativas, UDF, UDAF e UDTF, além de permitir o desenvolvimento de funções customizadas.\n",
    "\n",
    "UDF – Funções de usuário.\n",
    "- Uma entrada e uma saída. Ex. SPLIT\n",
    "\n",
    "UDAF – Funções de agregação.\n",
    "- N entradas e uma saída. Ex. SUM\n",
    "\n",
    "UDTF – Funções de tabelas.\n",
    "- Uma entrada e N saídas. Ex. EXPLODE\n",
    "\n",
    "## Função - SPLIT\n",
    "Recebe um valor e um delimitador, devolve um array, ou seja, entra um elemento e sai N, no mesmo campo.\n",
    "\n",
    "    \n",
    "    \n",
    "## Função - EXPLODE\n",
    "Recebe um array, ou seja, um conjunto de valores delimitados, e devolve em linhas.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d180f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c915fd1",
   "metadata": {},
   "source": [
    "## Hue\n",
    "Interface visual para manipulação do Hive e HDFS.\n",
    "\n",
    "    http://quickstart.cloudera:8888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d41ba01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b273c09",
   "metadata": {},
   "source": [
    "# SerDe\n",
    "- Abreviação de Serializer / Deserializer. \n",
    "- O Hive usa a interface SerDe para IO (Input/Output)\n",
    "- Um SerDe permite que o Hive leia os dados de uma tabela e os grave no HDFS em qualquer formato personalizado.\n",
    "- Qualquer um pode escrever seu próprio SerDe para seus próprios formatos de dados. \n",
    "- Avro, ORC, RegEx, Parquet, CSV, JsonSerDe.\n",
    "\n",
    "## Formatos de arquivos – Posicional \n",
    "Formato posicional utilizando SerDe. Crie um arquivo com o conteúdo abaixo no local do hdfs.\n",
    "\n",
    "Arquivo:\n",
    "~~~\n",
    "00001cliente_01PF\n",
    "00002cliente_02PJ\n",
    "00003cliente_03PF\n",
    "~~~\n",
    "\n",
    "### Hive\n",
    "~~~\n",
    "create external table tabela_posicional ( conta string, nome string, tipo_pessoa string ) \n",
    "row format serde 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe' \n",
    "with SERDEPROPERTIES ('input.regex' ='(.{5})(.{10})(.{2})') \n",
    "location '/user/hive/tabela_posicional';\n",
    "~~~\n",
    "\n",
    "### Databricks\n",
    "\n",
    "    DROP TABLE IF EXISTS tabela_posicional;\n",
    "    create table tabela_posicional ( conta string, nome string, tipo_pessoa string ) \n",
    "    row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe' \n",
    "    with SERDEPROPERTIES ('input.regex' ='(.{5})(.{10})(.{2})') \n",
    "    stored as textfile\n",
    "    location '/FileStore/tables/tabela_posicional'\n",
    "\n",
    "## Formatos de arquivos - Delimitado \n",
    "Formato posicional utilizando delimitador pipe. Crie um arquivo com o conteúdo abaixo no local do hdfs.\n",
    "\n",
    "Arquivo:\n",
    "~~~\n",
    "1,cliente_01,PF\n",
    "2,cliente_02,PJ\n",
    "3,cliente_03,PF\n",
    "~~~\n",
    "\n",
    "### Hive\n",
    "~~~\n",
    "create external table tabela_delimitada( conta string, nome string, tipo_pessoa string ) \n",
    "row format delimited fields terminated by ',' lines terminated by '\\n' stored as textfile \n",
    "location '/user/hive/tabela_delimitada';\n",
    "~~~\n",
    "\n",
    "### Databricks\n",
    "\n",
    "    create external table tabela_delimitada( conta string, nome string, tipo_pessoa string ) \n",
    "    row format delimited fields terminated by ',' lines terminated by '\\n' stored as textfile \n",
    "    location '/FileStore/tables/tabela_delimitada';\n",
    "\n",
    "\n",
    "## Formatos de arquivos – JSON\n",
    "JSON é um modelo para armazenamento de dados com capacidade de estruturar informações de uma forma bem mais compacta do que os arquivos sequenciais ou delimitados.\n",
    "\n",
    "Crie a tabela: \n",
    "~~~\n",
    "CREATE EXTERNAL TABLE tabela_json( campo string) stored as textfile LOCATION '/user/hive/tabela_json'; \n",
    "~~~\n",
    "\n",
    "Crie um arquivo no HDFS /tabela_json com o conteúdo:\n",
    "~~~\n",
    "{\"conta\":1,\"cliente\":{\"nome\":\"cliente_1\",\"tipo_pessoa\":\"PF\"}}\n",
    "{\"conta\":2,\"cliente\":{\"nome\":\"cliente_2\",\"tipo_pessoa\":\"PJ\"}}\n",
    "{\"conta\":3,\"cliente\":{\"nome\":\"cliente_3\",\"tipo_pessoa\":\"PF\"}}\n",
    "~~~\n",
    "\n",
    "Execute a query:\n",
    "~~~\n",
    "select get_json_object(campo, \"$.conta\") as conta, get_json_object(campo, \"$.cliente.nome\") as nome, get_json_object(campo, \"$.cliente.tipo_pessoa\") as tipo_pessoa from tabela_json;\n",
    "~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4288b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43773858",
   "metadata": {},
   "source": [
    "# Particionamento no Hive\n",
    "O Apache Hive organiza tabelas em partições. O particionamento é uma maneira de dividir uma tabela em partes relacionadas com base nos valores de colunas específicas, como data, cidade e departamento. Cada tabela na seção pode ter uma ou mais chaves de partição para identificar uma partição específica.\n",
    "\n",
    "Características: \n",
    "- Particiona a tabela em diretórios no HDFS \n",
    "- Ganho de performance e eficiência \n",
    "- Evita a leitura da base toda \n",
    "- Muito utilizado em grandes volumes de dados\n",
    "\n",
    "1. Vamos criar uma tabela particionada.\n",
    "\n",
    "    create external table table_part(nome string) \n",
    "    partitioned by (ano int)\n",
    "    row format delimited fields terminated by ',' \n",
    "    lines terminated by '\\n'\n",
    "    stored as textfile;\n",
    "\n",
    "\n",
    "2. Verificar tabela.\n",
    "\n",
    "    describe table_part;\n",
    "\n",
    "\n",
    "3. Adicionando partição.\n",
    "\n",
    "    alter table table_part \n",
    "    add partition(ano=2021) \n",
    "    location '/user/hive/table_part/particao/ano=2021' \n",
    "\n",
    "\n",
    "4. Excluindo partição.\n",
    "\n",
    "    alter table table_part drop partition(ano=2021);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2395b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97b4a8f5",
   "metadata": {},
   "source": [
    "# Extração de dados Hive - HDFS\n",
    "\n",
    "Agora vamos extrair dados do hive para hdfs.\n",
    "\n",
    "    insert overwrite directory '/user/hive/output' select * from teste_load;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a08268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c391ff32",
   "metadata": {},
   "source": [
    "\n",
    "# Impala\n",
    "O Hive não foi desenhado para executar queries em real time, com baixa latência, mas sim para melhor performance analisando grandes quantidades de dados que se encontram em clusters.\n",
    "\n",
    "Cloudera lançou o Impala, permitindo execução de queries com baixa latência para dados armazenados no HDFS e no HBase que, usando a linguagem SQL, permite integração com ferramentas de BI como Tableau, Microstrategy, Pentaho, etc.\n",
    "\n",
    "## Comandos Impala\n",
    "Execute comando abaixo para abrir shell do impala\n",
    "~~~\n",
    "impala-shell\n",
    "~~~\n",
    "\n",
    "## Hue\n",
    "O Impala também pode ser utilizado através do HUE, com interface visual, assim como o Hive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a172b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "234px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
